{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Made by KukJinKim\n",
    "\n",
    "\n",
    "## Minecraft RL 환경 설치\n",
    "1. 마인크래프트 게임은 Java Platform에서 동작합니다. 때문에 먼저 JDK 1.8 버전을 설치해야 합니다. \n",
    "\n",
    "(Windows)  \n",
    "https://www.oracle.com/kr/java/technologies/javase/javase8-archive-downloads.html\n",
    "\n",
    "(Mac)  \n",
    "brew tap AdoptOpenJDK/openjdk  \n",
    "brew install --cask adoptopenjdk8  \n",
    "\n",
    "(Ubuntu)  \n",
    "sudo add-apt-repository ppa:openjdk-r/ppa  \n",
    "sudo apt-get update  \n",
    "sudo apt-get install openjdk-8-jdk  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. pytorch가 설치된 가상환경에서 아래 매직커맨드를 실행하고 gym과 minerl을 설치해주세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.19.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from gym==0.19.0) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from gym==0.19.0) (1.19.5)\n",
      "Requirement already satisfied: minerl==0.3.7 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: gym>=0.13.1 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (0.19.0)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (4.5.5.62)\n",
      "Requirement already satisfied: setuptools>=40.6.2 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (58.0.4)\n",
      "Requirement already satisfied: tqdm>=4.32.2 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (4.63.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (1.19.5)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (2.27.1)\n",
      "Requirement already satisfied: ipython>=7.5.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (7.32.0)\n",
      "Requirement already satisfied: lxml>=4.3.3 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (4.8.0)\n",
      "Requirement already satisfied: psutil>=5.6.2 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (5.9.0)\n",
      "Requirement already satisfied: Pyro4>=4.76 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (4.82)\n",
      "Requirement already satisfied: coloredlogs>=10.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (15.0.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (3.5.1)\n",
      "Requirement already satisfied: dill>=0.3.1.1 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from minerl==0.3.7) (0.3.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from coloredlogs>=10.0->minerl==0.3.7) (10.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from gym>=0.13.1->minerl==0.3.7) (1.6.0)\n",
      "Requirement already satisfied: pyreadline in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs>=10.0->minerl==0.3.7) (2.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from ipython>=7.5.0->minerl==0.3.7) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from ipython>=7.5.0->minerl==0.3.7) (0.4.4)\n",
      "Requirement already satisfied: pygments in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from ipython>=7.5.0->minerl==0.3.7) (2.11.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from ipython>=7.5.0->minerl==0.3.7) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from ipython>=7.5.0->minerl==0.3.7) (5.1.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from ipython>=7.5.0->minerl==0.3.7) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from ipython>=7.5.0->minerl==0.3.7) (3.0.28)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from ipython>=7.5.0->minerl==0.3.7) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from ipython>=7.5.0->minerl==0.3.7) (5.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from jedi>=0.16->ipython>=7.5.0->minerl==0.3.7) (0.8.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from matplotlib>=3.2.2->minerl==0.3.7) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from matplotlib>=3.2.2->minerl==0.3.7) (4.29.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from matplotlib>=3.2.2->minerl==0.3.7) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from matplotlib>=3.2.2->minerl==0.3.7) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from matplotlib>=3.2.2->minerl==0.3.7) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from matplotlib>=3.2.2->minerl==0.3.7) (3.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from matplotlib>=3.2.2->minerl==0.3.7) (21.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.5.0->minerl==0.3.7) (0.2.5)\n",
      "Requirement already satisfied: serpent>=1.27 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from Pyro4>=4.76->minerl==0.3.7) (1.40)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->minerl==0.3.7) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from requests>=2.20.0->minerl==0.3.7) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from requests>=2.20.0->minerl==0.3.7) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from requests>=2.20.0->minerl==0.3.7) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages (from requests>=2.20.0->minerl==0.3.7) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.19.0\n",
    "!pip install minerl==0.3.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화학습 구현 팁 \n",
    "잘 모르는 강화학습 환경에서 어떤 모델을 구현하고 학습시킬 때는 보통 다음의 절차를 따릅니다.  \n",
    "1. MDP 및 환경 정보 확인\n",
    "2. Random Policy 구현 및 동작 확인\n",
    "3. 모델 구현\n",
    "4. 모델 동작 확인\n",
    "5. 버퍼 구현 (Replay Buffer, Temporal Buffer 등)\n",
    "6. 모델 업데이트 코드 구현\n",
    "7. 하이퍼파라미터 조정\n",
    "8. 반복 실험  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MDP 및 환경 정보 확인 \n",
    "먼저 MineRL의 환경 정보를 확인해보겠습니다.  \n",
    "환경의 정보라하면 크게 observation space, action space, dynamics, rewards, terminal state를 말합니다.  \n",
    "\n",
    "튜토리얼에서는 MineRLNavigateDense-v0 환경을 이용할 것입니다. 자세한 정보는 아래의 링크에서 확인할 수 있습니다.  \n",
    "https://minerl.readthedocs.io/en/latest/environments/index.html  \n",
    "\n",
    "\n",
    "아래의 코드를 실행시키면 마인크래프트 게임이 실행됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs space: Dict(compassAngle:Box(low=-180.0, high=180.0, shape=()), inventory:Dict(dirt:Box(low=0, high=2304, shape=())), pov:Box(low=0, high=255, shape=(64, 64, 3)))\n",
      "action space: Dict(attack:Discrete(2), back:Discrete(2), camera:Box(low=-180.0, high=180.0, shape=(2,)), forward:Discrete(2), jump:Discrete(2), left:Discrete(2), place:Enum(dirt,none), right:Discrete(2), sneak:Discrete(2), sprint:Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import minerl\n",
    "\n",
    "env = gym.make(\"MineRLNavigateDense-v0\")\n",
    "env.make_interactive(port=5656, realtime=False) # 상호작용을 위한 코드입니다. \n",
    "print(f\"obs space: {env.observation_space}\")\n",
    "print(f\"action space: {env.action_space}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드가 정상적으로 실행이 되었다면 아래와 같이 출력됩니다. \n",
    "### Env Info  \n",
    "obs space:  \n",
    "Dict(compassAngle:Box(low=-180.0, high=180.0, shape=()), inventory:Dict(dirt:Box(low=0, high=2304, shape=())), pov:Box(low=0, high=255, shape=(64, 64, 3)))  \n",
    "  \n",
    "\n",
    "action space:  \n",
    "Dict(attack:Discrete(2), back:Discrete(2), camera:Box(low=-180.0, high=180.0, shape=(2,)), forward:Discrete(2), jump:Discrete(2), left:Discrete(2), place:Enum(dirt,none), right:Discrete(2), sneak:Discrete(2), sprint:Discrete(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Goal  \n",
    "\n",
    "![](2022-06-28-15-31-33.png)\n",
    "![](2022-06-28-15-32-02.png)  \n",
    "본 환경의 목표는 에이전트가 다이아몬드 블록의 위치를 찾아가는 것입니다. 다이아몬드에 가까워질 수록 (+) 보상을 받고, 멀어질수록 (-) 보상을 받습니다. 에이전트가 다이아몬드 블록을 밝거나 600초를 넘기게 되면 에피소드가 종료됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1 Observation space  \n",
    "환경의 observation 정보는 64x64x3 RGB image tensor와  -180에서 180 사이의 스칼라 값입니다. 목표를 달성하기 위해 두 정보를 이용해야합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![obs_spec](./2022-06-28-15-28-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2 Action space\n",
    "\n",
    "attack:Discrete(2),  \n",
    "back:Discrete(2),  \n",
    "camera:Box(low=-180.0, high=180.0, shape=(2,)), forward:Discrete(2),  \n",
    "jump:Discrete(2),  \n",
    "left:Discrete(2),  \n",
    "place:Enum(dirt,none),  \n",
    "right:Discrete(2),  \n",
    "sneak:Discrete(2),  \n",
    "sprint:Discrete(2)\n",
    "\n",
    "에이전트가 환경에서 취할 수 있는 행동은 위와 같이 매우 다양합니다. 위 행동들의 조합을 만들어서 task를 수행하게끔 해야합니다. 모델의 복잡도를 줄이기 위해서 7개의 action만 사용할 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random policy 구현 및 동작 확인\n",
    "이제 위의 정보를 이용해서 환경에서 에이전트가 어떻게 동작하는지 확인하기 위해 Random Policy를 구현할 것입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def make_7action(env, action_index):\n",
    "    # Action들을 정의\n",
    "    action = env.action_space.noop()\n",
    "\n",
    "    # Always attack\n",
    "    action['attack'] = 1\n",
    "    action['jump'] = 0\n",
    "\n",
    "    # No action\n",
    "    if (action_index == 0):\n",
    "        action['camera'] = [0, 0]\n",
    "        action['forward'] = 0\n",
    "        action['jump'] = 0\n",
    "\n",
    "    # Camera\n",
    "    elif (action_index == 1):\n",
    "        action['camera'] = [0, -10]\n",
    "    elif (action_index == 2):\n",
    "        action['camera'] = [0, 10]\n",
    "    elif (action_index == 3):\n",
    "        action['camera'] = [-10, 0]\n",
    "    elif (action_index == 4):\n",
    "        action['camera'] = [10, 0]\n",
    "\n",
    "    # Move forward or jump\n",
    "    elif (action_index == 5):\n",
    "        action['forward'] = 1\n",
    "    elif (action_index == 6):\n",
    "        action['jump'] = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "def random_policy():\n",
    "    action_index = random.randint(0, 7) \n",
    "    action = make_7action(env, action_index)\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 5656 with Minecraft 1.11\n",
      "c:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:434: UserWarning: [WinError -2147417850] 스레드 모드가 설정된 후에는 바꿀 수 없습니다\n",
      "  warnings.warn(str(err))\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29936\\925694674.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# if __name__ == '__main__':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# main()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29936\\925694674.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 에이전트의 실행을 눈으로 볼 수 있습니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\minerl\\env\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    687\u001b[0m         if mode == 'human' and (\n\u001b[0;32m    688\u001b[0m                 not 'AICROWD_IS_GRADING' in os.environ or os.environ['AICROWD_IS_GRADING'] is None):\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_renderObs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_ac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_pov\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\minerl\\env\\core.py\u001b[0m in \u001b[0;36m_renderObs\u001b[1;34m(self, obs, ac)\u001b[0m\n\u001b[0;32m    680\u001b[0m                     cum_rewards=cum_rewards)\n\u001b[0;32m    681\u001b[0m         \u001b[1;31m# Todo: support more information to the render\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\minerl\\viewer\\trajectory_display.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, obs, reward, done, action, step, max)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;31m# Custom render loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_transparency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\pyglet\\gl\\win32.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0m_gdi32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vsync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 기본적인 gym의 실행과정과 똑같습니다. \n",
    "def main():\n",
    "    episodes = 2\n",
    "    for e in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            env.render() # 에이전트의 실행을 눈으로 볼 수 있습니다. \n",
    "            action = random_policy()\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                #print(f\"Episode {e} is finished\")\n",
    "                #print(f\"Total score: {score}\")\n",
    "                break\n",
    "    env.close()\n",
    "    return 0\n",
    "\n",
    "main() \n",
    "# if __name__ == '__main__':\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위 코드를 실행하고 아래의 매직커맨드를 터미널에서 실행하면 환경에 직접 접속할 수 있습니다.  \n",
    "\n",
    "![](2022-06-28-15-53-51.png)\n",
    "\n",
    "python -m minerl.interactor 5656"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모델 구현  \n",
    "이제 PPO 모델을 구현해보겠습니다. PPO 클래스와 observation numpy 배열을 torch tensor로 바꾸어주는 converter 함수를 구현해야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def navigate_converter(observation, device):\n",
    "    # Convert pixels\n",
    "    pixels = observation['pov']\n",
    "    pixels = torch.from_numpy(pixels).float() # 64, 64, 3\n",
    "    pixels /= 255.0 # int2float\n",
    "    pixels = pixels.permute(2, 0, 1) # 3, 64, 64\n",
    "    if len(pixels.shape) < 4: # Add batch dimension to pixels\n",
    "        pixels = pixels.unsqueeze(0) # 1, 3, 64, 64\n",
    "    \n",
    "    # Convert angle\n",
    "    angle = np.array([observation['compassAngle']], dtype=np.float)\n",
    "    compassAngle = torch.from_numpy(angle)\n",
    "    compassAngle /= 180.0\n",
    "    compassAngle = compassAngle.unsqueeze(0)\n",
    "    return pixels.to(device, dtype=torch.float), compassAngle.to(device, dtype=torch.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PPO Class  \n",
    "아키텍처는 다음의 그림과 같습니다.   \n",
    "![](2022-06-28-16-00-17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(PPO, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten()\n",
    "        )\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        conv_size = conv2d_size_out(64, 8, 4)\n",
    "        conv_size = conv2d_size_out(conv_size, 4, 2)\n",
    "        conv_size = conv2d_size_out(conv_size, 3, 1)\n",
    "        linear_input_size = conv_size * conv_size * 64 # 4 x 4 x 64 = 1024\n",
    "        self.fc = nn.Linear(linear_input_size+1, 512)\n",
    "        self.fc_pi = nn.Linear(512, self.num_actions)\n",
    "        self.fc_v = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, obs, softmax_dim=1):\n",
    "        # make_batch 코드에서 obs를 잘 만들어야 한다. \n",
    "        # pixels (Batch, C, H, W)\n",
    "        # angles (Batch, angle)\n",
    "        pixels, compassAngle = obs\n",
    "        conv_feature = self.conv_layers(pixels) # (Batch, Linear_size)\n",
    "        concat_feature = torch.cat((conv_feature, compassAngle), dim=1)\n",
    "        feature = F.relu(self.fc(concat_feature))\n",
    "        prob = self.fc_pi(feature)\n",
    "        log_prob = F.softmax(prob, dim=softmax_dim)\n",
    "        value = self.fc_v(feature)\n",
    "        return log_prob, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 동작 확인  \n",
    "이제 구현한 PPO 클래스의 동작을 확인해보겠습니다.  \n",
    "PPO 모델은 converter를  input을 받고 action_index를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "MineRL agent is public, connect on port 5656 with Minecraft 1.11\n",
      "c:\\Users\\ye200\\anaconda3\\envs\\minerl\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:434: UserWarning: [WinError -2147417850] 스레드 모드가 설정된 후에는 바꿀 수 없습니다\n",
      "  warnings.warn(str(err))\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to delete the temporary minecraft directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import minerl\n",
    "\n",
    "env = gym.make(\"MineRLNavigateDense-v0\")\n",
    "env.make_interactive(port=5656, realtime=False)\n",
    "\n",
    "def main():\n",
    "    model = PPO(num_actions=7).to(device)\n",
    "    episodes = 1\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            env.render() \n",
    "            # 1. actinon sampling \n",
    "            obs = navigate_converter(state, device)\n",
    "            prob, value = model(obs, softmax_dim=1)\n",
    "            prob = prob.squeeze(0) # prob 텐서의 배치 차원을 제거합니다. \n",
    "            m = Categorical(prob)\n",
    "            action_index = m.sample().item()\n",
    "            \n",
    "            # 2. convert action \n",
    "            action = make_7action(env, action_index)\n",
    "\n",
    "            # 3. take an action and get next information \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_obs = navigate_converter(next_state, device)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                #print(f\"Episode {e} is finished\")\n",
    "                #print(f\"Total score: {score}\")\n",
    "                break\n",
    "    env.close()\n",
    "    return 0\n",
    "\n",
    "main()\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 버퍼 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffer Class  \n",
    "make_batch의 함수에서 주목해야할 것은  \n",
    "1) 스칼라 값들을 리스트로 감싸서 torch.tensor로 만들어준 것\n",
    "2) 픽셀들을 torch.cat을 통해 배치 차원을 따라 합쳐주는 것입니다.  \n",
    "\n",
    "위 과정들은 update 함수에서 버퍼의 샘플들을 배치 텐서로 만들기 위함입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, T_horizon):\n",
    "        self.T_horizon = T_horizon\n",
    "        self.data = deque(maxlen=T_horizon)\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        # obs : pixels, angle\n",
    "        # trans (obs, a, r, next_obs, prob[a].item(), done)\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        pixels_lst, angles_lst = [], []\n",
    "        a_lst, r_lst, prob_a_lst, done_lst = [], [], [], []\n",
    "        n_pixels_lst, n_angles_lst = [], []\n",
    "\n",
    "        for transition in self.data:\n",
    "            obs, a, r, next_obs, prob_a, done = transition\n",
    "            pixels_lst.append(obs[0])\n",
    "            angles_lst.append([obs[1]])\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            n_pixels_lst.append(next_obs[0])\n",
    "            n_angles_lst.append([next_obs[1]])\n",
    "            prob_a_lst.append([prob_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "        \n",
    "        pixels = torch.cat(pixels_lst).to(device)\n",
    "        angles = torch.tensor(angles_lst).to(device)\n",
    "        a = torch.tensor(a_lst, dtype=torch.int64).to(device)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float).to(device)\n",
    "        n_pixels = torch.cat(n_pixels_lst).to(device)\n",
    "        n_angles = torch.tensor(n_angles_lst).to(device)\n",
    "        done_mask = torch.tensor(done_lst, dtype=torch.float).to(device)\n",
    "        prob_a = torch.tensor(prob_a_lst).to(device)\n",
    "        self.data = deque(maxlen=self.T_horizon)\n",
    "        obs = (pixels, angles)\n",
    "        next_obs = (n_pixels, n_angles)\n",
    "        return obs, a, r, next_obs, done_mask, prob_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. model update 코드 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_net function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(model, buffer, optimizer, K_epoch, lmbda, gamma, eps_clip, entopy_coef):\n",
    "    obs, a, r, next_obs, done_mask, prob_a = buffer.make_batch()\n",
    "    for i in range(K_epoch):\n",
    "        next_log_prob, next_value = model(next_obs)\n",
    "        pi, value = model(obs) # [batch, 4]\n",
    "\n",
    "        td_target = r + gamma * next_value * done_mask\n",
    "        delta = td_target - value\n",
    "        delta = delta.detach().cpu().numpy()\n",
    "\n",
    "        advantage_lst = []\n",
    "        advantage = 0.0\n",
    "        for delta_t in delta[::-1]:\n",
    "            advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "            advantage_lst.append([advantage])\n",
    "        advantage_lst.reverse()\n",
    "        advantage = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "\n",
    "        pi_a = pi.gather(1,a) \n",
    "        ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "        m = Categorical(pi)\n",
    "        entropy = m.entropy().mean()\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "        loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(value , td_target.detach()) - entopy_coef * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 학습 및 하이퍼파라미터 조정 \n",
    "이제 본격적으로 모델을 학습시켜보겠습니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def save_model(episode, SAVE_PERIOD, SAVE_PATH, model, MODEL_NAME, ENV_NAME):\n",
    "    if episode % SAVE_PERIOD == 0:\n",
    "        save_path_name = SAVE_PATH + ENV_NAME+'_'+MODEL_NAME+'_'+str(episode)+'.pt'\n",
    "        torch.save(model.state_dict(), save_path_name)\n",
    "        print(\"model saved\")\n",
    "\n",
    "def load_model(model, SAVE_PATH, MODEL_NAME):\n",
    "    model.load_state_dict(torch.load(SAVE_PATH+MODEL_NAME+'.pt'))\n",
    "    print(\"load model successfully\")\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVE_PATH = \"./weights/\"\n",
    "summary_path = \"./experiments/train/\"\n",
    "if not os.path.isdir(summary_path):\n",
    "    os.mkdir(summary_path)\n",
    "if not os.path.isdir(SAVE_PATH):\n",
    "    os.mkdir(SAVE_PATH)\n",
    "\n",
    "MODEL_NAME = 'PPO'\n",
    "ENV_NAME = 'MineRLNavigateDense-v0'\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.99\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "entopy_coef = 0.1\n",
    "K_epoch       = 5\n",
    "T_horizon     = 40\n",
    "\n",
    "SAVE_PERIOD = 100\n",
    "total_episodes = 2000\n",
    "print_interval = 20\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.11129617691040039\n",
      "# of episode :0, score : 43.9\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n",
      "0it [11:55, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.035930655896663666\n",
      "# of episode :1, score : 21.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.23664569854736328\n",
      "# of episode :2, score : 48.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.029654482379555702\n",
      "# of episode :3, score : -16.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.24434776604175568\n",
      "# of episode :4, score : 25.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.21240167319774628\n",
      "# of episode :5, score : 18.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.20293287932872772\n",
      "# of episode :6, score : -2.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.19111189246177673\n",
      "# of episode :7, score : 5.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.4168812334537506\n",
      "# of episode :8, score : 31.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.12589131295681\n",
      "# of episode :9, score : -9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.1534474641084671\n",
      "# of episode :10, score : 40.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.2577904164791107\n",
      "# of episode :11, score : -5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.31792280077934265\n",
      "# of episode :12, score : 40.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : -0.2872028946876526\n",
      "# of episode :13, score : -8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MineRL agent is public, connect on port 6666 with Minecraft 1.11\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    torch.manual_seed(3407)\n",
    "    writer = SummaryWriter(summary_path)\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env.make_interactive(port=6666, realtime=False)\n",
    "    model = PPO(num_actions=7).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    buffer = Buffer(T_horizon)\n",
    "\n",
    "    for n_epi in range(total_episodes):\n",
    "        score = 0.0\n",
    "        seed = 3407\n",
    "        env.seed(seed)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        loss = 0.0\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            steps += 1\n",
    "            for t in range(T_horizon):\n",
    "                env.render()\n",
    "                obs = navigate_converter(state, device)\n",
    "                prob, value = model(obs, softmax_dim=1)\n",
    "                prob = prob.squeeze(0)\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                action = make_7action(env, a)\n",
    "                s_prime, r, done, info = env.step(action)\n",
    "                next_obs = navigate_converter(s_prime, device)\n",
    "                transition = obs, a, r, next_obs, prob[a].item(), done\n",
    "                buffer.put_data(transition)\n",
    "                state = s_prime\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "            if done:\n",
    "                writer.add_scalar(\"total_rewards\", score, n_epi)\n",
    "                writer.add_scalar(\"loss\", loss, n_epi)\n",
    "                print(f'loss : {loss}')\n",
    "                print(\"# of episode :{}, score : {:.1f}\".format(n_epi, score))\n",
    "                break\n",
    "                   \n",
    "            loss = train_net(model, buffer, optimizer, K_epoch, lmbda, gamma, eps_clip, entopy_coef)\n",
    "        if n_epi % 100 == 0:\n",
    "            save_model(n_epi, SAVE_PERIOD, SAVE_PATH, model, MODEL_NAME, ENV_NAME)\n",
    "    writer.close()\n",
    "    env.close()\n",
    "    return 0\n",
    "\n",
    "main()\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('minerl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "266dbc1b53b3a75132c42213de358c469c42e7e8486df6091cb08cba7b9be0d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
